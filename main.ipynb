# ========================================================================
# 1. Installation and Environment Setup
# ========================================================================
!pip install hazm transformers[torch] scikit-learn pandas seaborn matplotlib tqdm -q

import os, torch, pandas as pd, numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm.auto import tqdm
from hazm import Normalizer, WordTokenizer, Lemmatizer, stopwords_list
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from transformers import AutoTokenizer, AutoModel

# Plotting configurations for professional output
plt.style.use('seaborn-v0_8-muted')
sns.set_context("talk")

# ========================================================================
# 2. Dataset Ingestion and Engineering
# ========================================================================
KAG_TOKEN = ""
URL = "https://www.kaggle.com/api/v1/datasets/download/amirzenoozi/persian-news-dataset"

print("Ingesting Dataset...")
if not os.path.exists('archive_v5.csv'):
    !curl -L -u :$KAG_TOKEN $URL -o data.zip
    !unzip -o data.zip

df = pd.read_csv('archive_v5.csv')
df = df.rename(columns={'service': 'category'}) # Standardize column name
df = df.dropna(subset=['body', 'category']).drop_duplicates(subset=['body'])

# Balanced sampling to prevent bias
df = df.groupby('category').apply(lambda x: x.sample(min(len(x), 800))).reset_index(drop=True)

# ========================================================================
# 3. Advanced NLP Preprocessing Engine
# ========================================================================
class AdvancedNewsProcessor:
    def __init__(self):
        self.normalizer = Normalizer()
        self.tokenizer = WordTokenizer()
        self.lemmatizer = Lemmatizer()
        self.stop_words = set(stopwords_list() + ['خبرگزاری', 'خبر', 'ایران', 'گزارش'])
    
    def __call__(self, text):
        if not isinstance(text, str): return ""
        # Normalize -> Tokenize -> Lemmatize -> Stopword removal
        norm = self.normalizer.normalize(text)
        tokens = self.tokenizer.tokenize(norm)
        return " ".join([self.lemmatizer.lemmatize(t).split('#')[0] 
                         for t in tokens if t not in self.stop_words and len(t) > 2])

print("Running NLP Engine...")
processor = AdvancedNewsProcessor()
df['cleaned_body'] = df['body'].apply(processor)

# ========================================================================
# 4. Feature Extraction using ParsBERT [CLS] Strategy
# ========================================================================
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Computation Device: {device}")

def extract_bert_features(texts, batch_size=16):
    tokenizer = AutoTokenizer.from_pretrained("HooshvareLab/bert-fa-zwnj-base")
    model = AutoModel.from_pretrained("HooshvareLab/bert-fa-zwnj-base").to(device)
    model.eval()
    
    cls_embeddings = []
    for i in tqdm(range(0, len(texts), batch_size), desc="BERT Encoding"):
        batch = texts[i : i + batch_size]
        inputs = tokenizer(batch, padding=True, truncation=True, max_length=128, return_tensors="pt").to(device)
        with torch.no_grad():
            outputs = model(**inputs)
            # Extract [CLS] token embedding as per project requirements
            cls_embeddings.append(outputs.last_hidden_state[:, 0, :].cpu().numpy())
    return np.vstack(cls_embeddings)

bert_features = extract_bert_features(df['cleaned_body'].tolist())

# ========================================================================
# 5. Classification Models Implementation (SVM vs BERT)
# ========================================================================
print("Preparing Classification Models...")

# Identify and remove classes with insufficient samples for stratified split
class_counts = df['category'].value_counts()
valid_categories = class_counts[class_counts >= 5].index
mask = df['category'].isin(valid_categories)

df_filtered = df[mask].copy()
bert_features_filtered = bert_features[mask.values]

# Stratified train-test split
X_train_raw, X_test_raw, y_train, y_test = train_test_split(
    df_filtered['cleaned_body'], df_filtered['category'], 
    test_size=0.2, stratify=df_filtered['category'], random_state=42
)

X_train_bert, X_test_bert, _, _ = train_test_split(
    bert_features_filtered, df_filtered['category'], 
    test_size=0.2, stratify=df_filtered['category'], random_state=42
)

# Model 1: Baseline (TF-IDF + Linear SVM)
tfidf = TfidfVectorizer(max_features=3000)
X_train_tfidf = tfidf.fit_transform(X_train_raw)
X_test_tfidf = tfidf.transform(X_test_raw)
svm = SVC(kernel='linear', probability=True).fit(X_train_tfidf, y_train)

# Model 2: SOTA (Logistic Regression on BERT CLS)
bert_classifier = LogisticRegression(max_iter=1000).fit(X_train_bert, y_train)

# ========================================================================
# 6. Unsupervised Clustering and Dimensionality Reduction
# ========================================================================
print("Analyzing Patterns with K-Means...")
num_clusters = df_filtered['category'].nunique()
kmeans = KMeans(n_clusters=num_clusters, n_init=10, random_state=42)
df_filtered['cluster'] = kmeans.fit_predict(bert_features_filtered)

# Dimensionality reduction for visualization
pca = PCA(n_components=2)
pca_data = pca.fit_transform(bert_features_filtered)

# ========================================================================
# 7. Professional Visualization Dashboard
# ========================================================================
print("\n" + "*" * 20 + " FINAL ANALYTICS DASHBOARD " + "*" * 20)

fig = plt.figure(figsize=(22, 12))
gs = fig.add_gridspec(2, 2)

# BERT Confusion Matrix
ax1 = fig.add_subplot(gs[0, 0])
sns.heatmap(confusion_matrix(y_test, bert_classifier.predict(X_test_bert)), 
            annot=True, fmt='d', cmap='magma', ax=ax1)
ax1.set_title("ParsBERT + Logistic Regression (Confusion Matrix)")

# PCA Cluster Scatter Plot
ax2 = fig.add_subplot(gs[0, 1])
scatter = ax2.scatter(pca_data[:, 0], pca_data[:, 1], c=df_filtered['cluster'], cmap='Spectral', s=40, alpha=0.7)
ax2.set_title("Latent Space Visualization (BERT + PCA)")
plt.colorbar(scatter, ax=ax2)

# Model Comparison Table
ax3 = fig.add_subplot(gs[1, :])
ax3.axis('off')
results_data = [
    ["Metric", "Baseline (SVM)", "SOTA (ParsBERT)"],
    ["Accuracy", f"{svm.score(X_test_tfidf, y_test):.2%}", f"{bert_classifier.score(X_test_bert, y_test):.2%}"],
    ["F1-Score (Weighted)", 
     f"{f1_score(y_test, svm.predict(X_test_tfidf), average='weighted'):.2%}", 
     f"{f1_score(y_test, bert_classifier.predict(X_test_bert), average='weighted'):.2%}"]
]
table = ax3.table(cellText=results_data, loc='center', cellLoc='center', colWidths=[0.2, 0.3, 0.3])
table.set_fontsize(14)
table.scale(1.2, 2.5)

plt.tight_layout()
plt.show()

# ========================================================================
# 8. Cluster Profiling via Mean TF-IDF
# ========================================================================
print("\nCLUSTER PROFILING (Top 10 TF-IDF Words per Cluster):")
full_tfidf = tfidf.fit_transform(df_filtered['cleaned_body'])
words = tfidf.get_feature_names_out()
for i in range(num_clusters):
    idx = df_filtered[df_filtered['cluster'] == i].index
    # Map original indices to matrix positions
    current_cluster_pos = [df_filtered.index.get_loc(k) for k in idx]
    if current_cluster_pos:
        scores = full_tfidf[current_cluster_pos].mean(axis=0).A1
        top_words = [words[j] for j in scores.argsort()[-10:][::-1]]
        print(f"Cluster {i:02d}: {' | '.join(top_words)}")
